"""Rolling Horizon ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«.

å›ºå®šã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã§ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—ã€æ™‚ç³»åˆ—é †åºã‚’ç¶­æŒã—ã¾ã™ã€‚

Rolling Horizonã®ç‰¹å¾´:
    - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºãŒä¸€å®šï¼ˆbatch_unitï¼‰
    - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºãŒä¸€å®šï¼ˆhorizonï¼‰
    - æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é¡ã£ã¦åˆ†å‰²å¯èƒ½ï¼ˆlatest_first=trueï¼‰
    
ä¾‹ï¼ˆbatch_unit=200, horizon=5ï¼‰:
    Fold 1: Train [0:200]    Test [200:205]
    Fold 2: Train [5:205]    Test [205:210]
    Fold 3: Train [10:210]   Test [210:215]

å…¸å‹çš„ãªä½¿ç”¨ä¾‹:
    $ python src/core/data_splitter.py \\
        --config data/experiments/TSLA_experiment.yaml
"""

import argparse
import json
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd
import yaml


def load_config(config_path: str) -> Dict[str, Any]:
    """å®Ÿé¨“è¨­å®šYAMLã‚’èª­ã¿è¾¼ã‚€.

    Args:
        config_path: å®Ÿé¨“è¨­å®šYAMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹.

    Returns:
        è¨­å®šå†…å®¹ã®è¾æ›¸.
    """
    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def rolling_horizon_split(
    df: pd.DataFrame,
    batch_unit: int = 200,
    horizon: int = 5,
    latest_first: bool = True,
    save_dir: Optional[str] = None,
    date_column: str = "Date",
    stats_columns: Optional[List[str]] = None,
) -> List[Dict[str, pd.DataFrame]]:
    """Rolling Horizonæ–¹å¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰².

    Args:
        df: åˆ†å‰²å¯¾è±¡ã®DataFrame.
        batch_unit: å„è¨“ç·´ãƒãƒƒãƒã®ã‚µãƒ³ãƒ—ãƒ«æ•°.
        horizon: å„ãƒ†ã‚¹ãƒˆãƒãƒƒãƒã®ã‚µãƒ³ãƒ—ãƒ«æ•°.
        latest_first: æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é¡ã‚‹ã‹.
        save_dir: ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª.
        date_column: æ—¥ä»˜åˆ—ã®åå‰.
        stats_columns: çµ±è¨ˆã‚’è¨ˆç®—ã™ã‚‹åˆ—åã®ãƒªã‚¹ãƒˆ.

    Returns:
        åˆ†å‰²çµæœã®è¾æ›¸ã®ãƒªã‚¹ãƒˆ.
    """
    total_samples = len(df)
    folds = []
    fold_num = 1

    if latest_first:
        # æœ€æ–°ã‹ã‚‰é¡ã‚‹
        end_idx = total_samples
        while end_idx >= batch_unit + horizon:
            start_idx = end_idx - batch_unit

            train_data = df.iloc[start_idx:end_idx]
            test_data = df.iloc[end_idx : end_idx + horizon]

            folds.append({"train": train_data, "test": test_data, "fold": fold_num})

            # Foldè©³ç´°ãƒ­ã‚°ã‚’å‰Šé™¤
            fold_num += 1
            end_idx -= horizon
    else:
        # å¤ã„ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é€²ã‚€
        start_idx = 0
        while start_idx + batch_unit + horizon <= total_samples:
            end_idx = start_idx + batch_unit

            train_data = df.iloc[start_idx:end_idx]
            test_data = df.iloc[end_idx : end_idx + horizon]

            folds.append({"train": train_data, "test": test_data, "fold": fold_num})

            # Foldè©³ç´°ãƒ­ã‚°ã‚’å‰Šé™¤
            fold_num += 1
            start_idx += horizon

    # æˆåŠŸã—ãŸç·Foldæ•°ã‚’è¡¨ç¤º
    print(f"\nâœ… Successfully created {len(folds)} folds")
    print(f"   Train size per fold: {batch_unit}")
    print(f"   Test size per fold: {horizon}")

    # ä¿å­˜å‡¦ç†
    if save_dir is not None:
        save_dir = Path(save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)

        for fold in folds:
            fold_dir = save_dir / f"fold_{fold['fold']}"
            fold_dir.mkdir(parents=True, exist_ok=True)

            # CSVä¿å­˜
            fold["train"].to_csv(fold_dir / "train.csv", index=False)
            fold["test"].to_csv(fold_dir / "test.csv", index=False)

            # çµ±è¨ˆæƒ…å ±
            train_stats = compute_stats(fold["train"], stats_columns)
            test_stats = compute_stats(fold["test"], stats_columns)

            stats = {"fold": fold["fold"], "train": train_stats, "test": test_stats}

            with open(fold_dir / "stats.json", "w", encoding="utf-8") as f:
                json.dump(stats, f, indent=2, ensure_ascii=False)

    return folds


def compute_stats(df: pd.DataFrame, stats_columns: list) -> Dict[str, Any]:
    """ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—.

    Args:
        df: çµ±è¨ˆã‚’è¨ˆç®—ã™ã‚‹DataFrame.
        stats_columns: çµ±è¨ˆã‚’è¨ˆç®—ã™ã‚‹åˆ—åã®ãƒªã‚¹ãƒˆ.

    Returns:
        çµ±è¨ˆæƒ…å ±ã®è¾æ›¸.
    """
    stats = {"n_samples": len(df)}

    # æ—¥ä»˜æƒ…å ±
    if "Date" in df.columns:
        stats["start_date"] = df["Date"].min().strftime("%Y-%m-%d")
        stats["end_date"] = df["Date"].max().strftime("%Y-%m-%d")

    # æ•°å€¤åˆ—ã®çµ±è¨ˆ
    for col in stats_columns:
        if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):
            stats[f"{col}_mean"] = float(df[col].mean())
            stats[f"{col}_std"] = float(df[col].std())
            stats[f"{col}_min"] = float(df[col].min())
            stats[f"{col}_max"] = float(df[col].max())

            # Sharpe-likeæŒ‡æ¨™ï¼ˆReturnsãŒã‚ã‚‹å ´åˆï¼‰
            if col == "Returns" and df[col].std() != 0:
                stats["sharpe_like"] = float(df[col].mean() / df[col].std() * np.sqrt(252))

    return stats


def print_fold_info(fold_idx: int, train_df: pd.DataFrame, test_df: pd.DataFrame) -> None:
    """Foldæƒ…å ±ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›.

    Args:
        fold_idx: Foldã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹.
        train_df: è¨“ç·´ãƒ‡ãƒ¼ã‚¿.
        test_df: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿.
    """
    if "Date" in train_df.columns:
        train_start = train_df["Date"].min().strftime("%Y-%m-%d")
        train_end = train_df["Date"].max().strftime("%Y-%m-%d")
        test_start = test_df["Date"].min().strftime("%Y-%m-%d")
        test_end = test_df["Date"].max().strftime("%Y-%m-%d")

        print(f"[Fold {fold_idx}] Train: {train_start} ~ {train_end} (N={len(train_df)})")
        print(f"[Fold {fold_idx}] Test:  {test_start} ~ {test_end} (N={len(test_df)})")
    else:
        print(f"[Fold {fold_idx}] Train: N={len(train_df)}")
        print(f"[Fold {fold_idx}] Test:  N={len(test_df)}")


def run_split(config: Dict[str, Any]) -> None:
    """åˆ†å‰²å‡¦ç†ã‚’å®Ÿè¡Œ."""
    ticker = config["ticker"]
    split_config = config["split"]

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    input_path = split_config["input_data"]
    print(f"ğŸ“‚ Loading data: {input_path}")
    df = pd.read_csv(input_path)

    # æ—¥ä»˜åˆ—ã®å‡¦ç†
    date_column = split_config.get("date_column", "Date")
    if date_column in df.columns:
        df[date_column] = pd.to_datetime(df[date_column])
        df = df.sort_values(date_column).reset_index(drop=True)

    print(f"\n{'=' * 60}")
    print(f"Rolling Horizon Split: {ticker}")
    print(f"  Batch Unit: {split_config['batch_unit']}")
    print(f"  Horizon: {split_config['horizon']}")
    print(f"  Latest First: {split_config.get('latest_first', True)}")
    print(f"{'=' * 60}\n")

    # åˆ†å‰²å®Ÿè¡Œ (dfã‚’æ¸¡ã™)
    _ = rolling_horizon_split(
        df=df,  # â† ã“ã‚ŒãŒå¿…è¦
        batch_unit=split_config["batch_unit"],
        horizon=split_config["horizon"],
        latest_first=split_config.get("latest_first", True),
        save_dir=split_config["save_dir"],
        date_column=date_column,
        stats_columns=split_config.get("stats_columns", ["Returns", "Close"]),
    )

    print(f"\nâœ… All splits saved to: {split_config['save_dir']}")


def main() -> None:
    """CLIã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ."""
    parser = argparse.ArgumentParser(
        description="Rolling Horizon ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã‚’å®Ÿè¡Œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--config",
        type=str,
        required=True,
        help="å®Ÿé¨“è¨­å®šYAMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆä¾‹: data/experiments/TSLA_experiment.yamlï¼‰",
    )

    args = parser.parse_args()

    # è¨­å®šèª­ã¿è¾¼ã¿
    config = load_config(args.config)

    # åˆ†å‰²å®Ÿè¡Œ
    run_split(config)


if __name__ == "__main__":
    main()
